{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Cancer Observations with Spark Machine Learning\n",
    "The data in use is from the <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)\"><b>Wisconsin Diagnostic Breast Cancer (WDBC) Data Set</b></a> which categorizes breast tumor cases as either benign or malignant based on 9 features to predict the diagnosis. For each cancer observation, we have the following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Sample code number: id number<br/>\n",
    "2\\. Clump Thickness: 1 - 10<br/>\n",
    "3\\. Uniformity of Cell Size: 1 - 10<br/>\n",
    "4\\. Uniformity of Cell Shape: 1 - 10<br/>\n",
    "5\\. Marginal Adhesion: 1 - 10<br/>\n",
    "6\\. Single Epithelial Cell Size: 1 - 10<br/>\n",
    "7\\. Bare Nuclei: 1 - 10<br/>\n",
    "8\\. Bland Chromatin: 1 - 10<br/>\n",
    "9\\. Normal Nucleoli: 1 - 10<br/>\n",
    "10\\. Mitoses: 1 - 10<br/>\n",
    "11\\. Class: (2 for benign, 4 for malignant)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cancer Observation data file has the following format:<br/>\n",
    "1000025,5,1,1,1,2,1,3,1,1,2<br/>\n",
    "1002945,5,4,4,5,7,10,3,2,1,2<br/>\n",
    "1015425,3,1,1,1,2,2,3,1,1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, we will build a logistic regression model to predict the label / classification of malignant or not based on the following features:\n",
    "<ul>\n",
    "    <li>Label → malignant or benign (1 or 0)</li>\n",
    "    <li>Features → {Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses }</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML used in this example\n",
    "<img src=\"img/bcmlprocess.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Parse the Data\n",
    "Import the machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext = org.apache.spark.sql.SQLContext@688ed34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@688ed34"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "import sqlContext._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.mllib.linalg.DenseVector\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a Scala case class to define the schema corresponding to a line in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Obs\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** define the Cancer Observation Schema */\n",
    "case class Obs(\n",
    "    clas: Double,\n",
    "    thickness: Double,\n",
    "    size: Double,\n",
    "    shape: Double,\n",
    "    madh: Double,\n",
    "    epsize: Double,\n",
    "    bnuc: Double,\n",
    "    bchrom: Double,\n",
    "    nNuc: Double,\n",
    "    mit: Double\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below parse a line from the data file into the Cancer Observation class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseObs: (line: Array[Double])Obs\n",
       "parseRDD: (rdd: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[Array[Double]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/* \n",
    " * function to create a Obs class from an Array of Double.\n",
    " * Class Malignant 4 is changed to 1\n",
    " */\n",
    "def parseObs(line: Array[Double]): Obs = {\n",
    "    Obs(\n",
    "        if (line(9) == 4.0) 1 else 0, line(0), line(1), line(2), line(3), line(4), line(5), line(6), line(7), line(8)\n",
    "    )\n",
    "}\n",
    "\n",
    "/* \n",
    " * function to transform an RDD of Strings into an RDD of Double,\n",
    " * filter lines with ?, remove first column\n",
    " */\n",
    "def parseRDD(rdd: RDD[String]): RDD[Array[Double]] = {\n",
    "    rdd.map(_.split(\",\"))\n",
    "       .filter(_(6) != \"?\")\n",
    "       .map(_.drop(1))\n",
    "       .map(_.map(_.toDouble))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the data from the csv file into an RDD of Strings. Then we use the map transformation on the rdd, which will apply the ParseRDD function to transform each String element in the RDD into an Array of Double. Then we use another map transformation, which will apply the ParseObs function to transform each Array of Double in the RDD into an Array of Cancer Observation objects. The toDF() method transforms the RDD of Array[[Cancer Observation]] into a Dataframe with the Cancer Observation class schema.\n",
    "<img src=\"img/bcloaddata.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = data/breast-cancer-wisconsin.data MapPartitionsRDD[25] at textFile at <console>:62\n",
       "obsRDD = MapPartitionsRDD[30] at map at <console>:63\n",
       "obsDF = [clas: double, thickness: double ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[clas: double, thickness: double ... 8 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load the data into a DataFrame\n",
    "val rdd = sc.textFile(\"data/breast-cancer-wisconsin.data\")\n",
    "val obsRDD = parseRDD(rdd).map(parseObs)\n",
    "val obsDF = obsRDD.toDF().cache()\n",
    "obsDF.registerTempTable(\"obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame printSchema() Prints the schema to the console in a tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clas: double (nullable = false)\n",
      " |-- thickness: double (nullable = false)\n",
      " |-- size: double (nullable = false)\n",
      " |-- shape: double (nullable = false)\n",
      " |-- madh: double (nullable = false)\n",
      " |-- epsize: double (nullable = false)\n",
      " |-- bnuc: double (nullable = false)\n",
      " |-- bchrom: double (nullable = false)\n",
      " |-- nNuc: double (nullable = false)\n",
      " |-- mit: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Return the schema of this DataFrame\n",
    "obsDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+-----+----+------+----+------+----+---+\n",
      "|clas|thickness|size|shape|madh|epsize|bnuc|bchrom|nNuc|mit|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+\n",
      "| 0.0|      5.0| 1.0|  1.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|\n",
      "| 0.0|      5.0| 4.0|  4.0| 5.0|   7.0|10.0|   3.0| 2.0|1.0|\n",
      "| 0.0|      3.0| 1.0|  1.0| 1.0|   2.0| 2.0|   3.0| 1.0|1.0|\n",
      "| 0.0|      6.0| 8.0|  8.0| 1.0|   3.0| 4.0|   3.0| 7.0|1.0|\n",
      "| 0.0|      4.0| 1.0|  1.0| 3.0|   2.0| 1.0|   3.0| 1.0|1.0|\n",
      "| 1.0|      8.0|10.0| 10.0| 8.0|   7.0|10.0|   9.0| 7.0|1.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0|10.0|   3.0| 1.0|1.0|\n",
      "| 0.0|      2.0| 1.0|  2.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|\n",
      "| 0.0|      2.0| 1.0|  1.0| 1.0|   2.0| 1.0|   1.0| 1.0|5.0|\n",
      "| 0.0|      4.0| 2.0|  1.0| 1.0|   2.0| 1.0|   2.0| 1.0|1.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   1.0| 1.0|   3.0| 1.0|1.0|\n",
      "| 0.0|      2.0| 1.0|  1.0| 1.0|   2.0| 1.0|   2.0| 1.0|1.0|\n",
      "| 1.0|      5.0| 3.0|  3.0| 3.0|   2.0| 3.0|   4.0| 4.0|1.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 3.0|   3.0| 1.0|1.0|\n",
      "| 1.0|      8.0| 7.0|  5.0|10.0|   7.0| 9.0|   5.0| 5.0|4.0|\n",
      "| 1.0|      7.0| 4.0|  6.0| 4.0|   6.0| 1.0|   4.0| 3.0|1.0|\n",
      "| 0.0|      4.0| 1.0|  1.0| 1.0|   2.0| 1.0|   2.0| 1.0|1.0|\n",
      "| 0.0|      4.0| 1.0|  1.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|\n",
      "| 1.0|     10.0| 7.0|  7.0| 6.0|   4.0|10.0|   4.0| 1.0|2.0|\n",
      "| 0.0|      6.0| 1.0|  1.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Display the top 20 rows of DataFrame\n",
    "obsDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query dataframe using SQL queries. The funtions are provided by the Scala DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         thickness|\n",
      "+-------+------------------+\n",
      "|  count|               683|\n",
      "|   mean|  4.44216691068814|\n",
      "| stddev|2.8207613188371288|\n",
      "|    min|               1.0|\n",
      "|    max|              10.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//  describe computes statistics for thickness column, including count, mean, stddev, min, and max\n",
    "obsDF.describe(\"thickness\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+------------------+------------------+\n",
      "|clas|     avgthickness|           avgsize|          avgshape|\n",
      "+----+-----------------+------------------+------------------+\n",
      "| 0.0|2.963963963963964|1.3063063063063063|1.4144144144144144|\n",
      "| 1.0|7.188284518828452| 6.577405857740586| 6.560669456066946|\n",
      "+----+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// compute the avg thickness, size, shape grouped by clas (malignant or not)\n",
    "sqlContext.sql(\"SELECT clas, avg(thickness) as avgthickness, avg(size) as avgsize, avg(shape) as avgshape FROM obs GROUP BY clas\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|clas|   avg(thickness)|\n",
      "+----+-----------------+\n",
      "| 0.0|2.963963963963964|\n",
      "| 1.0|7.188284518828452|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// compute avg thickness grouped by clas (malignant or not)\n",
    "obsDF.groupBy(\"clas\").avg(\"thickness\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features\n",
    "To build a classifier model, you first extract the features that most contribute to the classification. In the cancer data set, the data is labeled with two classes – 1 (malignant) and 0 (not malignant).\n",
    "\n",
    "The features for each item consists of the fields shown below:\n",
    "<ul>\n",
    "    <li>Label → malignant: 0 or 1</li>\n",
    "    <li>Features → {\"thickness\", \"size\", \"shape\", \"madh\", \"epsize\", \"bnuc\", \"bchrom\", \"nNuc\", \"mit\"}</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Features Array\n",
    "In order for the features to be used by a machine learning algorithm, the features are transformed and put into Feature Vectors, which are vectors of numbers representing the value for each feature.\n",
    "\n",
    "Below a VectorAssembler is used to transform and return a new DataFrame with all of the feature columns in a vector column\n",
    "<img src=\"img/bctransformfeatures.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+\n",
      "|clas|thickness|size|shape|madh|epsize|bnuc|bchrom|nNuc|mit|            features|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+\n",
      "| 0.0|      5.0| 1.0|  1.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|[5.0,1.0,1.0,1.0,...|\n",
      "| 0.0|      5.0| 4.0|  4.0| 5.0|   7.0|10.0|   3.0| 2.0|1.0|[5.0,4.0,4.0,5.0,...|\n",
      "| 0.0|      3.0| 1.0|  1.0| 1.0|   2.0| 2.0|   3.0| 1.0|1.0|[3.0,1.0,1.0,1.0,...|\n",
      "| 0.0|      6.0| 8.0|  8.0| 1.0|   3.0| 4.0|   3.0| 7.0|1.0|[6.0,8.0,8.0,1.0,...|\n",
      "| 0.0|      4.0| 1.0|  1.0| 3.0|   2.0| 1.0|   3.0| 1.0|1.0|[4.0,1.0,1.0,3.0,...|\n",
      "| 1.0|      8.0|10.0| 10.0| 8.0|   7.0|10.0|   9.0| 7.0|1.0|[8.0,10.0,10.0,8....|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0|10.0|   3.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|\n",
      "| 0.0|      2.0| 1.0|  2.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|[2.0,1.0,2.0,1.0,...|\n",
      "| 0.0|      2.0| 1.0|  1.0| 1.0|   2.0| 1.0|   1.0| 1.0|5.0|[2.0,1.0,1.0,1.0,...|\n",
      "| 0.0|      4.0| 2.0|  1.0| 1.0|   2.0| 1.0|   2.0| 1.0|1.0|[4.0,2.0,1.0,1.0,...|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featureCols = Array(thickness, size, shape, madh, epsize, bnuc, bchrom, nNuc, mit)\n",
       "assembler = vecAssembler_333a2c001e2b\n",
       "df2 = [clas: double, thickness: double ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[clas: double, thickness: double ... 9 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//define the feature columns to put in the feature vector\n",
    "val featureCols = Array(\"thickness\", \"size\", \"shape\", \"madh\", \"epsize\", \"bnuc\", \"bchrom\", \"nNuc\", \"mit\")\n",
    "\n",
    "//set the input and output column names\n",
    "val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n",
    "\n",
    "//return a dataframe with all of the  feature columns in  a vector column\n",
    "val df2 = assembler.transform(obsDF)\n",
    "\n",
    "// the transform method produced a new column: features.\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use a StringIndexer to return a Dataframe with the clas (malignant or not) column added as a label.\n",
    "<img src=\"img/bctransformfeaturesandlabel.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+-----+\n",
      "|clas|thickness|size|shape|madh|epsize|bnuc|bchrom|nNuc|mit|            features|label|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+-----+\n",
      "| 0.0|      5.0| 1.0|  1.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|[5.0,1.0,1.0,1.0,...|  0.0|\n",
      "| 0.0|      5.0| 4.0|  4.0| 5.0|   7.0|10.0|   3.0| 2.0|1.0|[5.0,4.0,4.0,5.0,...|  0.0|\n",
      "| 0.0|      3.0| 1.0|  1.0| 1.0|   2.0| 2.0|   3.0| 1.0|1.0|[3.0,1.0,1.0,1.0,...|  0.0|\n",
      "| 0.0|      6.0| 8.0|  8.0| 1.0|   3.0| 4.0|   3.0| 7.0|1.0|[6.0,8.0,8.0,1.0,...|  0.0|\n",
      "| 0.0|      4.0| 1.0|  1.0| 3.0|   2.0| 1.0|   3.0| 1.0|1.0|[4.0,1.0,1.0,3.0,...|  0.0|\n",
      "| 1.0|      8.0|10.0| 10.0| 8.0|   7.0|10.0|   9.0| 7.0|1.0|[8.0,10.0,10.0,8....|  1.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0|10.0|   3.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "| 0.0|      2.0| 1.0|  2.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|[2.0,1.0,2.0,1.0,...|  0.0|\n",
      "| 0.0|      2.0| 1.0|  1.0| 1.0|   2.0| 1.0|   1.0| 1.0|5.0|[2.0,1.0,1.0,1.0,...|  0.0|\n",
      "| 0.0|      4.0| 2.0|  1.0| 1.0|   2.0| 1.0|   2.0| 1.0|1.0|[4.0,2.0,1.0,1.0,...|  0.0|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "labelIndexer = strIdx_bfc548d6e197\n",
       "df3 = [clas: double, thickness: double ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[clas: double, thickness: double ... 10 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  Create a label column with the StringIndexer\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"clas\").setOutputCol(\"label\")\n",
    "val df3 = labelIndexer.fit(df2).transform(df2)\n",
    "\n",
    "// the  transform method produced a new column: label.\n",
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the data is split into a training data set and a test data set, 70% of the data is used to train the model, and 30% will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splitSeed = 5043\n",
       "trainingData = [clas: double, thickness: double ... 10 more fields]\n",
       "testData = [clas: double, thickness: double ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[clas: double, thickness: double ... 10 more fields]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  split the dataframe into training and test data\n",
    "val splitSeed = 5043\n",
    "val Array(trainingData, testData) = df3.randomSplit(Array(0.7, 0.3), splitSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "<img src=\"img/creditmlcrossvalidation.png\">\n",
    "Next, we train the logistic regression model with elastic net regularization\n",
    "\n",
    "The model is trained by making associations between the input features and the labeled output associated with those features.\n",
    "<img src=\"img/bcfitmodel.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.06503554553146344,0.07181362361391282,0.0,0.0,0.07583963853124735,0.0012675057388237378,0.0,0.0] Intercept: -1.393191423126092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr = logreg_a135e303a107\n",
       "model = logreg_a135e303a107\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "logreg_a135e303a107"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the classifier,  set parameters for training\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "// use logistic regression to train (fit) the model with the training data\n",
    "val model = lr.fit(trainingData)    \n",
    "\n",
    "// Print the coefficients and intercept for logistic regression**\n",
    "println(s\"Coefficients: ${model.coefficients} Intercept: ${model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model\n",
    "Next we use the test data to get predictions.\n",
    "<img src=\"img/bcpredicttest.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+-----+--------------------+--------------------+----------+\n",
      "|clas|thickness|size|shape|madh|epsize|bnuc|bchrom|nNuc|mit|            features|label|       rawPrediction|         probability|prediction|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+-----+--------------------+--------------------+----------+\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   1.0| 1.0|   1.0| 3.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[1.17923510971064...|[0.76481024658406...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   1.0| 1.0|   3.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[1.17670009823299...|[0.76435395397908...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   1.0| 1.0|   3.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[1.17670009823299...|[0.76435395397908...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 1.0|   1.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[1.17923510971064...|[0.76481024658406...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 1.0|   2.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[1.17796760397182...|[0.76458217679258...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 1.0|   3.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[1.17670009823299...|[0.76435395397908...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 2.0|   2.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[1.10212796544057...|[0.75065860985163...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 5.0|   1.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[0.87587655558565...|[0.70596701397664...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 5.0|   1.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[0.87587655558565...|[0.70596701397664...|       0.0|\n",
      "| 0.0|      1.0| 1.0|  1.0| 1.0|   2.0| 5.0|   5.0| 1.0|1.0|[1.0,1.0,1.0,1.0,...|  0.0|[0.87080653263036...|[0.70491349294414...|       0.0|\n",
      "+----+---------+----+-----+----+------+----+------+----+---+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions = [clas: double, thickness: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[clas: double, thickness: double ... 13 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// run the  model on test features to get predictions\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// As you can see, the previous model transform produced a new columns: rawPrediction, probablity and prediction.**\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we evaluate the predictions, we use a BinaryClassificationEvaluator which returns a precision metric by comparing the test label column with the test prediction column. In this case, the evaluation returns 99% precision.\n",
    "<img src=\"img/bcevaluatemodelpredictions.png\">\n",
    "A common metric used for logistic regression is area under the ROC curve (AUC). We can use the <b>BinaryClasssificationEvaluator</b> to obtain the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = binEval_cf70e1b1178d\n",
       "accuracy = 0.9926910299003322\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9926910299003322"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create an Evaluator for binary classification, which expects two input columns: rawPrediction and label.\n",
    "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "// Evaluates predictions and returns a scalar metric areaUnderROC(larger is better).**\n",
    "val accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we calculate some more metrics. The number of false and true positive and negative predictions is also useful:\n",
    "\n",
    "<ul>\n",
    "    <li>True positives are how often the model correctly predicted a tumour was malignant</li>\n",
    "    <li>False positives are how often the model predicted a tumour was malignant when it was benign</li>\n",
    "    <li>True negatives indicate how the model correctly predicted a tumour was benign</li>\n",
    "    <li>False negatives indicate how often the model predicted a tumour was benign when in fact it was malignant</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp = [label: double, prediction: double]\n",
       "counttotal = 199\n",
       "correct = 168\n",
       "wrong = 31\n",
       "truep = 128\n",
       "falseN = 30\n",
       "falseP = 1\n",
       "ratioWrong = 0.15577889447236182\n",
       "ratioCorrect = 0.8442211055276382\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8442211055276382"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate Metrics\n",
    "val lp = predictions.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 74.0 failed 1 times, most recent failure: Lost task 0.0 in stage 74.0 (TID 488, localhost, executor driver): java.lang.ClassCastException: org.apache.spark.ml.linalg.DenseVector cannot be cast to org.apache.spark.mllib.linalg.DenseVector\n",
       "\tat $line148.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:61)\n",
       "\tat $line148.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:61)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\n",
       "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat $anonfun$1.apply(<console>:61)\n",
       "\tat $anonfun$1.apply(<console>:61)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\n",
       "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n",
       "  at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n",
       "  at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n",
       "  at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n",
       "  at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n",
       "  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n",
       "  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n",
       "  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n",
       "  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n",
       "  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\n",
       "  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.pr(BinaryClassificationMetrics.scala:107)\n",
       "  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderPR(BinaryClassificationMetrics.scala:117)\n",
       "  ... 50 elided\n",
       "Caused by: java.lang.ClassCastException: org.apache.spark.ml.linalg.DenseVector cannot be cast to org.apache.spark.mllib.linalg.DenseVector\n",
       "  at $anonfun$1.apply(<console>:61)\n",
       "  at $anonfun$1.apply(<console>:61)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\n",
       "  at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// use MLlib to evaluate, convert DF to RDD\n",
    "val predictionAndLabels = predictions.select(\"rawPrediction\", \"label\").rdd.map(x => (x(0).asInstanceOf[DenseVector](1), x(1).asInstanceOf[Double]))\n",
    "val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "println(\"area under the precision-recall curve: \" + metrics.areaUnderPR)\n",
    "println(\"area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n",
    "\n",
    "// A Precision-Recall curve plots (precision, recall) points for different threshold values, while a receiver operating characteristic, or ROC, curve plots (recall, false positive rate) points. The closer  the area Under ROC is to 1, the better the model is making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
